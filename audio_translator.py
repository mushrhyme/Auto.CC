import os
import re
import io
import time
import json
import queue
import wave
import asyncio
import logging
import random
import tempfile
import threading
from datetime import datetime
from pathlib import Path

import pyaudio
import requests
import numpy as np
import soundfile as sf
import aiohttp
from langdetect import detect
from PyQt5.QtCore import QTimer
import backoff
import socket
import boto3
import websockets
from presigned_url import AWSTranscribePresignedURL
import subprocess
import sys
from eventstream import create_audio_event, decode_event
import string


# Constants
FORMAT = pyaudio.paInt16
CHANNELS = 1  
RATE = 8000  # ì´ˆë‹¹ ìˆ˜ì§‘í•˜ëŠ” ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜: ë†’ì„ìˆ˜ë¡ í’ˆì§ˆì´ ì¢‹ì•„ì§€ë‚˜ ë” ë§ì€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì•¼ í•¨(16000 = Whisper ê¶Œì¥ ìƒ˜í”Œë§ ë ˆì´íŠ¸)
CHUNK = 1024 # í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜:  ì‘ì„ìˆ˜ë¡ ë¹ ë¥´ê²Œ ì²˜ë¦¬ë˜ë‚˜ í’ˆì§ˆì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ
SILENCE_THRESHOLD = 600 # í‰ê·  ì§„í­ì´ ì´ ê°’ ë¯¸ë§Œì´ë©´ ì¹¨ë¬µìœ¼ë¡œ íŒë‹¨
SILENCE_DURATION = 2 # ì¹¨ë¬µ ì§€ì† ì‹œê°„: ì´ ì‹œê°„ ë™ì•ˆ ì¹¨ë¬µì´ë©´ ë°œí™”ê°€ ì¢…ë£Œëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
# REALTIME_UPDATE_INTERVAL = 1.0 # ì‹¤ì‹œê°„ ë²ˆì—­ ì—…ë°ì´íŠ¸ ê°„ê²©: ìŒì„±ì´ ì§„í–‰ ì¤‘ì¼ ë•Œ í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ë²„í¼ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì¤‘ê°„ ë²ˆì—­ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” ì‹œê°„ ê°„ê²©
MAX_SENTENCE_LENGTH = 50 # ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ (ììœ ë¡­ê²Œ ì¡°ì • ê°€ëŠ¥)
TARGET_LANGUAGES = {
    'ko': ['Chinese', 'Japanese', 'English'],
    'ja': ['Korean', 'Chinese', 'English'],
    'en': ['Korean', 'Chinese', 'Japanese'],
    'zh': ['Korean', 'Japanese', 'English']
}
# GPT_MODEL = "gpt-3.5-turbo"  
GPT_MODEL = "gpt-4o-mini-2024-07-18"

# API endpoints
TRANSCRIPTION_URL = "https://api.openai.com/v1/audio/transcriptions"
TRANSLATION_URL = "https://api.openai.com/v1/chat/completions"
# 172.17.17.82:8080

class AudioTranslator:
    def __init__(self):
        self.setup_logging()
        self.load_config()
        self.audio_folder = Path("audio")
        self.translation_mode = "aws" 
        self.audio_queue = queue.Queue()
        self.is_running = True
        self.voice_detected = False
        self.silence_count = 0
        self.update_interval = 1.0
        self.detected_language = None
        self.last_translation = ""
        self.audio_frames = []
        self.buffer_lock = threading.Lock()
    
        # API í‚¤ ì´ˆê¸°í™”
        self.api_key = os.environ.get("OPENAI_API_KEY")
        if not self.api_key:
            self.logger.error("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        
        if self.translation_mode == "aws":
            # # awscli ì„¤ì¹˜
            # try:
            #     import awscli
            #     print("\033[94mawscli already installed\033[0m")
            # except ImportError:
            #     print("\033[94mInstalling awscli...\033[0m")
            #     subprocess.check_call([sys.executable, "-m", "pip", "install", "awscli"])
            #     print("\033[94mawscli installation completed\033[0m")

            # awscli ì„¤ì¹˜ í›„ í™˜ê²½ë³€ìˆ˜ì—ì„œ AWS ìê²© ì¦ëª… ì½ê¸°
            aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')
            aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
            aws_session_token = os.environ.get('AWS_SESSION_TOKEN')

            # ì„¸ì…˜ í† í°ì´ ì—†ëŠ” ê²½ìš° aws sts get-session-tokenìœ¼ë¡œ ë™ì ìœ¼ë¡œ íšë“
            if not aws_session_token:
                output = subprocess.run(
                    ["aws", "sts", "get-session-token", "--duration-seconds", "3600", "--output", "json"],
                    capture_output=True, text=True
                )
                output_json = json.loads(output.stdout)
                creds = output_json.get("Credentials", {})
                aws_access_key_id = creds.get('AccessKeyId')
                aws_secret_access_key = creds.get('SecretAccessKey')
                aws_session_token = creds.get('SessionToken')

            self.aws_transcribe = AWSTranscribePresignedURL(
                aws_access_key_id,
                aws_secret_access_key,
                aws_session_token,
                region="ap-northeast-2"
            )
            self.aws_translate = boto3.session.Session(
                aws_access_key_id=aws_access_key_id,
                aws_secret_access_key=aws_secret_access_key,
                aws_session_token=aws_session_token,
                region_name="ap-northeast-2"
            ).client('translate')    
            # (A) AWS Transcribe ì–¸ì–´ ì½”ë“œ
            self.aws_language_code = "en-US" # en-US  ko-KR

            # (B) AWS Translate APIìš© ì†ŒìŠ¤/íƒ€fê²Ÿ ì½”ë“œ
            #    ex) "en", "ko" ë“±
            self.aws_source_lang_code = "en"
            self.aws_target_lang_code = "ko"

            # (C) GUIì— ë¿Œë¦´ ë•Œ ì–¸ì–´ëª… í‚¤
            #    ex) self.aws_target_lang_code == "ko" ì´ë©´ "Korean"
            self.translate_target_lang_name = "Korean"
            
            self.aws_stream_stop = threading.Event()
            self.use_silence_vad = True
            
            self.aws_url = self.aws_transcribe.get_request_url(
                sample_rate=RATE,
                language_code=self.aws_language_code,
                media_encoding="pcm",
                number_of_channels=1,
                # enable_partial_results_stabilization=True
            )
    # ---------- ì´ˆê¸°í™” ë° ì„¤ì • ----------
    def setup_logging(self):
        """ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì„±"""
        # ë¡œê·¸ íŒŒì¼ ì„¤ì •
        log_folder = Path("logs")
        log_folder.mkdir(exist_ok=True)

        # ë¡œê·¸ íŒŒì¼ ì´ë¦„ ì„¤ì •
        self.log_filename = log_folder / f"translation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

        # ë¡œê±° ì„¤ì •
        self.logger = logging.getLogger("TranslationLogger")
    
        # ì¤‘ìš”: ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ë¡œê·¸ ì¤‘ë³µ ë°©ì§€)
        if self.logger.handlers:
            for handler in self.logger.handlers[:]:
                self.logger.removeHandler(handler)
    
        self.logger.setLevel(logging.INFO)
    
        # íŒŒì¼ ë° ì½˜ì†” í•¸ë“¤ëŸ¬ ì„¤ì •
        file_handler = logging.FileHandler(self.log_filename, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
    
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
    
        # ë¡œê·¸ í˜•ì‹ ì„¤ì •
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
    
        # í•¸ë“¤ëŸ¬ ì¶”ê°€
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
    
    def load_config(self):
        """ê¸°ë³¸ê°’ ì„¤ì •"""
        # ê¸°ë³¸ ì„¤ì •
        self.config = {
            "silence_threshold": SILENCE_THRESHOLD,
            "silence_duration": SILENCE_DURATION,
            "preferred_device": 0,  # ê¸°ë³¸ ì¥ì¹˜ ì¸ë±ìŠ¤
        }
        
        # ê¸°ì¡´ ì„¤ì • ë¡œë“œ ì‹œë„
        self.silence_threshold = self.config["silence_threshold"]
        self.silence_duration = self.config["silence_duration"]
        self.selected_device = self.config["preferred_device"]  # ì¶”ê°€: selected_device ì„¤ì •
    
        # ì„¤ì •ì—ì„œ ë³€ìˆ˜ ì„¤ì •
        self.chunk_duration = CHUNK / RATE
        self.silence_chunks = int(self.silence_duration / self.chunk_duration)
        self.min_volume_for_display = 200

    # ---------- ì˜¤ë””ì˜¤ ì¥ì¹˜ ê´€ë¦¬ ----------
    def list_audio_devices(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì˜¤ë””ì˜¤ ì…ë ¥ ì¥ì¹˜ ë‚˜ì—´"""
        # PyAudioë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜¤ë””ì˜¤ ì¥ì¹˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        p = pyaudio.PyAudio()
        device_count = p.get_device_count()
    
        self.logger.info("\nAvailable audio input devices:")
        self.logger.info("-" * 50)
    
        # ëª¨ë“  ì¥ì¹˜ì— ëŒ€í•œ ì •ë³´ í‘œì‹œ
        devices = []
        for i in range(device_count):
            device_info = p.get_device_info_by_index(i)
            # ì…ë ¥ ì±„ë„ì´ ìˆëŠ” ì¥ì¹˜ë§Œ í‘œì‹œ
            if device_info.get('maxInputChannels') > 0:
                devices.append({
                    'index': i,
                    'name': device_info.get('name'),
                    'channels': device_info.get('maxInputChannels'),
                    'sample_rate': int(device_info.get('defaultSampleRate'))
                })
            
                self.logger.info(f"Device number {i}: {device_info.get('name')}")
                self.logger.info(f"  Channels: {device_info.get('maxInputChannels')}")
                self.logger.info(f"  Sample rate: {int(device_info.get('defaultSampleRate'))}")
                self.logger.info("-" * 50)
        # PyAudio ì •ë¦¬
        p.terminate()
        return devices

    def check_audio_input(self, device_index):
        """ì˜¤ë””ì˜¤ ì…ë ¥ ìƒíƒœ í™•ì¸"""
        p = pyaudio.PyAudio()
        try:
            device_info = p.get_device_info_by_index(device_index if device_index is not None else
                                                p.get_default_input_device_info()['index'])
        except Exception as e:
            self.logger.error(f"ì˜¤ë””ì˜¤ ì…ë ¥ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        finally:
            p.terminate()
    
    # ---------- ì˜¤ë””ì–´ ë°ì´í„° ì²˜ë¦¬ ----------
    @staticmethod
    def get_audio_level(audio_data):
        """ì˜¤ë””ì˜¤ ë°ì´í„°ì˜ ë³¼ë¥¨ ë ˆë²¨ ê³„ì‚°"""
        if len(audio_data) == 0:
            print("Audio data is empty.")  # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            return 0
    
        # ë” íš¨ìœ¨ì ì¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ numpy ë°°ì—´ë¡œ ë³€í™˜
        normalized = np.abs(np.frombuffer(audio_data, dtype=np.int16))
        if len(normalized) == 0:
            print("Normalized audio data is empty.")  # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            return 0

        # ë” ë‚˜ì€ ê°ì§€ë¥¼ ìœ„í•´ ìƒìœ„ 10% ìƒ˜í”Œ ì‚¬ìš©
        sorted_samples = np.sort(normalized)
        top_samples = sorted_samples[int(len(sorted_samples) * 0.8):]
    
        # ìƒìœ„ ìƒ˜í”Œì´ ì—†ìœ¼ë©´ ì „ì²´ ì‚¬ìš©
        if len(top_samples) > 0:
            return np.mean(top_samples)
        return np.mean(normalized)

    def should_transcribe(self, audio_level):
        """ì˜¤ë””ì˜¤ ë ˆë²¨ì— ë”°ë¼ ìŒì„±ì´ ê°ì§€ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        if audio_level > self.silence_threshold:
            self.silence_count = 0
        
            # ìŒì„± ê°ì§€ ìƒíƒœ ì—…ë°ì´íŠ¸
            if not self.voice_detected:
                self.voice_start_time = datetime.now()  # ìŒì„± ì‹œì‘ ì‹œê°„ ê¸°ë¡
                self.transcribe_start_time = self.voice_start_time
                self.logger.info(f"âœ… ë°œí™” ê°ì§€! Level: {audio_level:.1f}")    
                
                if self.translation_mode=="local":
                    # ê°ì§€ëœ ì–¸ì–´ë¥¼ ì—…ë°ì´íŠ¸
                    if hasattr(self, 'gui_signals'):
                        try:
                            # ìµœê·¼ ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                            with self.buffer_lock:
                                frames_copy = list(self.audio_frames[-10:])  # ìµœê·¼ 10ê°œì˜ í”„ë ˆì„ ë³µì‚¬
                            if frames_copy:
                                audio_file_path = self.save_audio_to_wav(frames_copy)
                                transcription, _ = self.transcribe_audio(audio_file_path, 8080)
                                if transcription is None:
                                    self.logger.info("ë°œí™” ë‚´ìš©ì´ ì—†ì–´ ë²ˆì—­ ìš”ì²­ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                                    return False  # ë²ˆì—­ ì¤‘ë‹¨
                                
                                if transcription:
                                    print(f"ë°œí™” ê°ì§€ëœ í…ìŠ¤íŠ¸: {transcription}")
                                    detected_language = detect(transcription)  # í…ìŠ¤íŠ¸ë¡œ ì–¸ì–´ ê°ì§€
                                    self.detected_language = 'zh' if 'zh' in detected_language else detected_language
                                    language_map = {
                                        "ko": "í•œêµ­ì–´",
                                        "en": "ì˜ì–´",
                                        "zh": "ì¤‘êµ­ì–´",
                                        "ja": "ì¼ë³¸ì–´"
                                    }
                                    language_name = language_map.get(detected_language, "ì•Œ ìˆ˜ ì—†ìŒ")
                                    self.gui_signals.status_update.emit(f"{language_name}ë¥¼ ê°ì§€ ì¤‘ì…ë‹ˆë‹¤...")
                                else:
                                    self.gui_signals.status_update.emit("ì–¸ì–´ ê°ì§€ ì‹¤íŒ¨")
                            else:
                                self.gui_signals.status_update.emit("ì˜¤ë””ì˜¤ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì–¸ì–´ ê°ì§€ ì‹¤íŒ¨")
                        except Exception as e:
                            self.logger.error(f"ì–¸ì–´ ê°ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                            self.gui_signals.status_update.emit("ì–¸ì–´ ê°ì§€ ì‹¤íŒ¨")

            self.voice_detected = True
            return True
        else:
            self.silence_count += 1
        
            # ì¹¨ë¬µ í›„ ìŒì„± ê°ì§€ ìƒíƒœ ì¬ì„¤ì •
            if self.silence_count > self.silence_chunks and self.voice_detected:
                # ë°œí™” ì¢…ë£Œ ë¡œê·¸ ì œê±° (ì¤‘ë³µ ë°©ì§€)
                self.logger.info("âœ… [ì‹¤ì‹œê°„] ë°œí™”ê°€ ì™„ì „íˆ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                self.voice_start_time = None
                self.transcribe_start_time = None
                self.voice_detected = False
        
            return False
    
    def save_audio_to_wav(self, frames, temp=True, channels=None):
        """ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ WAV íŒŒì¼ë¡œ ì €ì¥"""
        if not frames:
            return None

        if channels is None:
            channels = CHANNELS

        if temp:
            # ê¸°ì¡´ ì„ì‹œ íŒŒì¼ ë¡œì§
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
            filename = temp_file.name
            temp_file.close()
        else:
            # ì˜¤ë””ì˜¤ í´ë”ì— íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
            ts = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
            filename = str(self.audio_folder / f"audio_{ts}.wav")

        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(channels)
            wf.setsampwidth(2)
            wf.setframerate(RATE)
            wf.writeframes(b''.join(frames))

        return filename
    
    @backoff.on_exception(backoff.expo, (requests.exceptions.RequestException, Exception), max_tries=1)    
    def transcribe_audio(self, audio_file_path, ports):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë¡œì»¬ STT API ì‚¬ìš©)
        """
        flac_file_path = None
        try:     
            if self.translation_mode=="local":
                flac_file_path = self._convert_to_flac(audio_file_path)
                result = self._call_transcription_api(flac_file_path)
                transcription = result.get("text", "").strip()
                return (transcription or None), None
            
            elif self.translation_mode=="server":
                result = self._call_transcription_api(audio_file_path, ports)
                print(f"call_local_api -> result: {result}")
                transcription = result.get("original_text", "").strip()
                trans_text = result.get("trans_text", {})
                ori_lang = result.get("ori_language")
                
                # í‚¤(ko,zh,en) â†’ full name ë§¤í•‘
                lang_map = {'ko':'Korean','zh':'Chinese','en':'English'}
                translations = {v: '' for v in lang_map.values()}
                for code, txt in trans_text.items():
                    if code in lang_map:
                        translations[lang_map[code]] = txt.strip()
                # ì›ë¬¸ë„ í¬í•¨
                if ori_lang in lang_map:
                    translations[lang_map[ori_lang]] = transcription
                return (transcription or None), translations
            
            # ë°œí™” ê¸¸ì´ í™•ì¸
            if transcription and len(transcription.strip()) < 3:
                self.logger.info(f"ë°œí™”ê°€ ë„ˆë¬´ ì§§ì•„ ë²ˆì—­ì„ ê±´ë„ˆëœë‹ˆë‹¤: '{transcription}'")
                return None
            return transcription
        except Exception as e:
            self.logger.error(f"STT í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return None
        # finally:
        #     if mode=="local":
        #         self._cleanup_temp_files(audio_file_path, flac_file_path)
        #     else:
        #         self._cleanup_temp_files(audio_file_path) 

    def _convert_to_flac(self, audio_file_path):
        """WAV íŒŒì¼ì„ FLAC í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
        flac_file_path = audio_file_path.replace(".wav", ".flac")
        with sf.SoundFile(audio_file_path) as wav_file:
            data = wav_file.read(dtype='int16')
            sf.write(flac_file_path, data, wav_file.samplerate, format='FLAC')
        return flac_file_path

    def _call_transcription_api(self, file_path, ports=None):
        """API í˜¸ì¶œë¡œ í…ìŠ¤íŠ¸ ë³€í™˜"""
        # OpenAI APIë¥¼ ì„œë²„ì—ì„œ í˜¸ì¶œí•˜ëŠ” ê²½ìš°
        if self.translation_mode == "server":
            def is_port_open(host, port, timeout=1.0):
                try:
                    with socket.create_connection((host, port), timeout=timeout):
                        return True
                except socket.timeout:
                    self.logger.error(f"í¬íŠ¸ {port} ì—°ê²° ì‹œë„ ì¤‘ íƒ€ì„ì•„ì›ƒ ë°œìƒ")
                except ConnectionRefusedError:
                    self.logger.error(f"í¬íŠ¸ {port} ì—°ê²°ì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤")
                except Exception as e:
                    self.logger.error(f"í¬íŠ¸ {port} ì—°ê²° ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
                return False
                

            STT_SERVER_IP = "172.17.17.82"
            # 172.26.81.43
            # 172.25.1.95
            available_ports = [ports]
            # [port for port in ports if is_port_open(STT_SERVER_IP, port)]

            if not available_ports:
                self.logger.error("âš ï¸ ì—°ê²° ê°€ëŠ¥í•œ STT í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None

            port = random.choice(available_ports)
            url = f"http://{STT_SERVER_IP}:{port}/api/transcribe"

            try:
                with open(file_path, 'rb') as f:
                    files = {'audio_file': ('audio.wav', f, 'audio/wav')}
                    response = requests.post(url, files=files, timeout=20)
                    if response.status_code == 200:
                        result = response.json()
                        return result
                    else:
                        self.logger.error(f"STT API ì˜¤ë¥˜ {response.status_code}: {response.text}")
                        return None
            except Exception as e:
                self.logger.error(f"ë¡œì»¬ STT ì „ì†¡ ì‹¤íŒ¨: {e}", exc_info=True)
                return None
        # OpenAI APIë¥¼ ë¡œì»¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
        else:
            headers = {"Authorization": f"Bearer {self.api_key}"}
            with open(file_path, 'rb') as audio_file:
                files = {
                    'file': (os.path.basename(file_path), audio_file, 'audio/flac'),
                    'model': (None, 'whisper-1'),
                    'response_format': (None, 'json')
                }
                response = requests.post(TRANSCRIPTION_URL, headers=headers, files=files, timeout=20)

            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"Transcription API error: {response.status_code}, {response.text}", exc_info=True)
                return None

    def _cleanup_temp_files(self, *file_paths):
        """ì„ì‹œ íŒŒì¼ ì‚­ì œ"""
        for file_path in file_paths:
            if file_path and os.path.exists(file_path):
                try:
                    os.unlink(file_path)
                except Exception as e:
                    self.logger.error(f"Failed to delete temporary file: {e}", exc_info=True)
            
    def convert_to_flac(self, audio_data):
        """ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ ë‚´ì—ì„œ FLAC í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
        try:
            # audio_dataê°€ ë°”ì´íŠ¸ í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° ë°”ì´íŠ¸ë¡œ ë³€í™˜
            if isinstance(audio_data, str):
                audio_data = audio_data.encode('utf-8')

            # ìŒì„± ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            audio_np = np.frombuffer(audio_data, dtype=np.int16)

            # FLAC í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ íŒŒì¼-like ê°ì²´ ìƒì„±
            flac_buffer = io.BytesIO()
        
            # soundfileì„ ì‚¬ìš©í•´ numpy ë°ì´í„°ë¥¼ FLAC í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ë¡œ ì €ì¥
            with sf.SoundFile(flac_buffer, 'w', samplerate=RATE, channels=CHANNELS, format='FLAC') as file:
                file.write(audio_np)

            # ë©”ëª¨ë¦¬ì—ì„œ FLAC ë°ì´í„° ë°˜í™˜
            flac_buffer.seek(0)
            return flac_buffer.read()  # ì‹¤ì œ FLAC ë°ì´í„° ë°˜í™˜
        except Exception as e:
            self.logger.error(f"Error during FLAC conversion: {e}", exc_info=True)
        return None

    def audio_capture(self, device_index=None):
        """ì˜¤ë””ì˜¤ ìº¡ì²˜ ë° íì— ë°ì´í„° ì¶”ê°€"""
        if self.translation_mode=="aws":
            p = pyaudio.PyAudio()
            stream = self._open_audio_stream(p, device_index, CHANNELS)
            try:
                while self.is_running:
                    chunk = stream.read(CHUNK, exception_on_overflow=False)
                    # AWS ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„ê°€ êº¼ë‚´ê°€ë„ë¡ íì— ë„£ëŠ”ë‹¤
                    self.audio_queue.put(chunk)
            finally:
                stream.stop_stream()
                stream.close()
                p.terminate()
                
        else:
            self.check_audio_input(device_index)

            p = pyaudio.PyAudio()
            stream = None
            selected_channels = CHANNELS

            def log_audio_level(data):
                """ì˜¤ë””ì˜¤ ë ˆë²¨ì„ ë¡œê¹…í•˜ê³  GUIì— ì—…ë°ì´íŠ¸"""
                audio_level = self.get_audio_level(data)
                if audio_level > 400:
                    print(f"audio_level: {audio_level}")
                if hasattr(self, 'gui_signals'):
                    self.gui_signals.audio_level_update.emit(audio_level)

            try:
                # ì¥ì¹˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                if device_index is not None:
                    try:
                        device_info = p.get_device_info_by_index(device_index)
                        max_input_channels = int(device_info.get('maxInputChannels', 1))
                        selected_channels = min(selected_channels, max_input_channels)
                        self.logger.info(f"ì¥ì¹˜ ì •ë³´: {device_info.get('name')} (ì±„ë„: {selected_channels})")
                    except Exception as e:
                        self.logger.error(f"ì¥ì¹˜ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}", exc_info=True)
                        device_index = None

                # ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì—´ê¸°
                stream = self._open_audio_stream(p, device_index, selected_channels)

                if not stream:
                    self.logger.error("ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ì—´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìº¡ì²˜ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    return
                if self.translation_mode == "aws":
                    try:
                        while self.is_running and self.translation_mode == "aws":
                            data = stream.read(CHUNK, exception_on_overflow=False)
                            with self.buffer_lock:
                                self.audio_frames.append(data)
                    finally:
                        stream.stop_stream()
                        stream.close()
                        p.terminate()
                else:
                    self.logger.info(f"í˜„ì¬ ì¹¨ë¬µ ì„ê³„ê°’: {self.silence_threshold} (ì´ ê°’ ì´ìƒì´ë©´ ì˜¤ë””ì˜¤ê°€ ê°ì§€ë¨)")

                    # ì˜¤ë””ì˜¤ ìº¡ì²˜ ë£¨í”„
                    silence_counter = 0
                    speech_detected_during_session = False
                    volume_monitor_counter = 0
                    while self.is_running:
                        try:
                            data = stream.read(CHUNK, exception_on_overflow=False)
                            log_audio_level(data)

                            # í˜„ì¬ ì²­í¬ì˜ í‰ê·  ì§„í­ ê³„ì‚°
                            audio_level = self.get_audio_level(data)

                            # ë³¼ë¥¨ ë ˆë²¨ ëª¨ë‹ˆí„°ë§ (5ì´ˆë§ˆë‹¤)
                            volume_monitor_counter += 1
                            if volume_monitor_counter >= 80:  # 80 * 0.0625 = 5ì´ˆ
                                volume_monitor_counter = 0

                            # ë²„í¼ì— ë°ì´í„° ì¶”ê°€
                            with self.buffer_lock:
                                self.audio_frames.append(data)

                            # ìŒì„± ê°ì§€ í™•ì¸
                            voice_detect = self.should_transcribe(audio_level)
                            if voice_detect:
                                silence_counter = 0
                                speech_detected_during_session = True
                            else:
                                silence_counter += 1

                            # ì¹¨ë¬µì´ ì§€ì†ë˜ë©´ ì„¸ì…˜ ì²˜ë¦¬ ì¢…ë£Œ
                            if silence_counter >= self.silence_chunks and len(self.audio_frames) > 0:
                                for _ in range(3):  # ì•½ 0.3 ~ 0.4ì´ˆ ë¶„ëŸ‰ ë” ìˆ˜ì§‘
                                    try:
                                        extra_data = stream.read(CHUNK, exception_on_overflow=False)
                                        with self.buffer_lock:
                                            self.audio_frames.append(extra_data)
                                    except Exception as e:
                                        self.logger.error(f"ì¶”ê°€ ì˜¤ë””ì˜¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                                        
                                with self.buffer_lock:
                                    frames_copy = list(self.audio_frames)
                                    self.audio_frames.clear()

                                # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆê³  ìŒì„±ì´ ê°ì§€ëœ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                                min_frames = int((RATE * 1.5) / CHUNK)
                                if len(frames_copy) > min_frames and speech_detected_during_session:
                                    self.audio_queue.put((frames_copy, selected_channels))

                                silence_counter = 0
                                speech_detected_during_session = False

                        except Exception as e:
                            self.logger.error(f"ì˜¤ë””ì˜¤ ìº¡ì²˜ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤: {e}", exc_info=True)

            finally:
                if stream:
                    stream.stop_stream()
                    stream.close()
                p.terminate()

            
    def _open_audio_stream(self, p, device_index, channels):
        """ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ì—´ê³  ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì¥ì¹˜ ë˜ëŠ” ëª¨ë…¸ ì±„ë„ë¡œ ì‹œë„"""
        try:
            return p.open(
                format=FORMAT,
                channels=channels,
                rate=RATE,
                input=True,
                input_device_index=device_index,
                frames_per_buffer=CHUNK
            )
        except Exception as e:
            self.logger.error(f"ìŠ¤íŠ¸ë¦¼ ì—´ê¸° ì‹¤íŒ¨: {e}", exc_info=True)
            self.logger.info("ê¸°ë³¸ ì…ë ¥ ì¥ì¹˜ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")

            try:
                default_info = p.get_default_input_device_info()
                default_channels = min(CHANNELS, int(default_info.get('maxInputChannels', 1)))
                return p.open(
                    format=FORMAT,
                    channels=default_channels,
                    rate=RATE,
                    input=True,
                    frames_per_buffer=CHUNK
                )
            except Exception as e2:
                self.logger.error(f"ê¸°ë³¸ ì¥ì¹˜ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e2}", exc_info=True)
                self.logger.info("ëª¨ë…¸ ì±„ë„ë¡œ ë§ˆì§€ë§‰ ì‹œë„ë¥¼ í•©ë‹ˆë‹¤...")

                try:
                    return p.open(
                        format=FORMAT,
                        channels=1,
                        rate=RATE,
                        input=True,
                        frames_per_buffer=CHUNK
                    )
                except Exception as e3:
                    self.logger.error(f"ëª¨ë“  ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì—´ê¸° ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e3}", exc_info=True)
                    return None

    # ---------- ë²ˆì—­ ì²˜ë¦¬ ----------
    def update_target_languages(self, selected_languages):
        """ì„ íƒëœ ì–¸ì–´ì— ë”°ë¼ ë²ˆì—­ ëŒ€ìƒ ì–¸ì–´ë¥¼ ì—…ë°ì´íŠ¸"""
        self.target_languages = selected_languages
        
    async def translate_text_async(self, text, size=200):
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ë¹„ë™ê¸° ë²ˆì—­"""
        if not text or not text.strip():
            return None

        chunks = [text[i:i+size] for i in range(0, len(text), size)]
        all_translations = await asyncio.gather(*(self.translate_chunk(chunk) for chunk in chunks))
        return self._merge_translations(all_translations)

    def _merge_translations(self, all_translations):
        """ë²ˆì—­ ê²°ê³¼ ë³‘í•©"""
        final = {}
        for translation_set in all_translations:
            if translation_set:
                for lang, trans in translation_set.items():
                    final.setdefault(lang, []).append(trans)
        return {k: ' '.join(filter(None, v)) for k, v in final.items()}

    async def translate_chunk(self, chunk):
        try:
            self.detected_language = detect(chunk)
            targets = TARGET_LANGUAGES.get(self.detected_language[:2], [])
            results = await asyncio.gather(*(self.call_translation_api(chunk, t) for t in targets))
            return dict(zip(targets, results))
        except Exception as e:
            self.logger.error(f"Language detection error: {str(e)}")
            return None

    async def call_translation_api(self, chunk, target_lang):
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        prompt = f'Translate to {target_lang}: "{chunk}"'
        data = {
            "model": GPT_MODEL, 
            "messages": [
                {"role": "system", 
                "content": "You are a translator. Only provide the translation without any explanation."},
                {"role": "user", "content": f"Translate to {target_lang} only:\n{chunk}"},
                ]
            }

        async with aiohttp.ClientSession() as session:
            async with session.post(TRANSLATION_URL, headers=headers, json=data) as resp:
                if resp.status == 200:
                    json_data = await resp.json()
                    return json_data.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
                self.logger.error(f"API error {resp.status}: {await resp.text()}")
                return None
            
    def process_translation_result(self, translation, transcription, prev_translation, accumulated_text, speech_id):
        """ë²ˆì—­ ê²°ê³¼ ì²˜ë¦¬ ë° GUI ì—…ë°ì´íŠ¸"""
    
        # ìƒˆë¡œìš´ ë²ˆì—­ ì‹œì‘ ì‹œ ì´ˆê¸°í™”
        timestamp = datetime.now().strftime('%H:%M:%S')
    
        prev_translation, accumulated_text = self._update_translation_state(
            translation, prev_translation, accumulated_text, speech_id
        )

        gui_message = self._prepare_gui_message(translation, transcription)
        # GUI ì‹ í˜¸ ë°œì†¡
        if hasattr(self, 'gui_signals'):
            self.gui_signals.translation_update.emit(
                timestamp,
                json.dumps(gui_message),  # ë”•ì…”ë„ˆë¦¬ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
                transcription
            )

    def _update_translation_state(self, translation, prev_translation, accumulated_text, speech_id):
        """ë²ˆì—­ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        if not hasattr(self, 'translation_results'):
            self.translation_results = {}

        if speech_id not in self.translation_results:
            self.translation_results[speech_id] = {"prev_translation": "", "accumulated_text": ""}

        current_state = self.translation_results[speech_id]
        prev_translation = current_state["prev_translation"]
        accumulated_text = current_state["accumulated_text"]

        if not translation:
            return prev_translation, accumulated_text

        # ë²ˆì—­ ê²°ê³¼ì— ì–¸ì–´ë³„ ê¸°ë³¸ê°’ ì„¤ì •
        for lang in ["English", "Chinese", "Korean"]:
            translation.setdefault(lang, "")
        
        # í‚¤ê°€ ìˆëŠ” ì–¸ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        for lang, translated_text in translation.items():
            prev_trans = prev_translation.get(lang, "") if isinstance(prev_translation, dict) else prev_translation
            if len(accumulated_text) < MAX_SENTENCE_LENGTH:
                if prev_trans and translated_text.startswith(prev_trans):
                    # print(f"translation: {translation}")
                    new_text = translated_text[len(prev_trans):].strip()
                    if new_text:
                        accumulated_text[lang] += " " + new_text
                else:
                    accumulated_text = translation
            # print(f"accumulated_text: {accumulated_text}")
            self.translation_results[speech_id]["prev_translation"] = translation
            self.translation_results[speech_id]["accumulated_text"] = accumulated_text
            self.last_translation = accumulated_text
            
        return prev_translation, accumulated_text
            
    def _prepare_gui_message(self, translation, transcription):
        # print(f"translation: {translation}")
        """GUI ë©”ì‹œì§€ êµ¬ì„±"""
        gui_message = {
            "korean": translation.get("Korean", ""),
            "english": translation.get("English", ""),
            "chinese": translation.get("Chinese", ""),
            # "japanese": translation.get("Japanese", "")
        }
        # print(f"gui_message: {gui_message}")
        return gui_message

    async def process_translation_queue(self):
        """ë²ˆì—­ í ì²˜ë¦¬"""
        current_speech_id = 0

        while self.is_running:
            frames_copy, selected_channels = await self._get_audio_from_queue()
            if frames_copy is None:
                continue

            current_speech_id += 1
            try:
                await self.handle_audio_frames(frames_copy, selected_channels, current_speech_id)
                self.audio_queue.task_done()
            except Exception as e:
                self.logger.error(f"Error in translation processing: {e}", exc_info=True)

    async def _get_audio_from_queue(self):
        """ì˜¤ë””ì˜¤ íì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.audio_queue.get(timeout=0.01)
        except queue.Empty:
            await asyncio.sleep(0.01)
        return None, None
    
    async def _aws_streaming_transcribe(self, wav_path):
        ws = await websockets.connect(self.aws_url, ping_timeout=None)
        try:
            full_transcript = ""
            last_len = 0

            async def send_audio():
                wf = wave.open(wav_path, 'rb')
                try:
                    chunk_size = int(RATE / 10) 
                    while True:
                        data = wf.readframes(chunk_size)
                        if not data:
                            break
                        await ws.send(create_audio_event(data))
                        await asyncio.sleep(0.1)
                finally:
                    wf.close()

            async def receive_transcript():
                nonlocal full_transcript, last_len
                try:
                    while True:
                        raw = await ws.recv()
                        header, payload = decode_event(raw)
                        if header.get(':message-type') == 'event':
                            results = payload['Transcript']['Results']
                            if results:
                                alt = results[0]['Alternatives'][0]
                                text = alt['Transcript']
                                if not results[0].get('IsPartial', False):
                                    if len(text) > last_len:
                                        full_transcript += text[last_len:]
                                        last_len = len(text)
                except websockets.exceptions.ConnectionClosedOK:
                    pass
                return full_transcript

            send_task = asyncio.create_task(send_audio())
            recv_task = asyncio.create_task(receive_transcript())

            # 1) ì˜¤ë””ì˜¤ ì „ì†¡ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
            await send_task
            # 2) ë°›ì€ ê²°ê³¼ê°€ ë‹¤ ëª¨ì¼ ë•Œê¹Œì§€ ëŒ€ê¸°
            transcription = await recv_task
            print("AWS STT ì„±ê³µ:", transcription)
            return transcription

        finally:
            # send/receiveê°€ ëë‚¬ë“  ì—ëŸ¬ë‚¬ë“  ë°˜ë“œì‹œ ì—°ê²° í•´ì œ
            await ws.close()

    async def handle_audio_frames(self, frames, channels, speech_id):
        """ì˜¤ë””ì˜¤ í”„ë ˆì„ ì²˜ë¦¬"""
        audio_file_path = self.save_audio_to_wav(frames, temp=False, channels=channels)

        self.log_timing_on_end_of_speech() # ë°œí™” ì¢…ë£Œ ì‹œê°„ ë¡œê¹…
        
        # ë²ˆì—­ ì‹œì‘ ì‹ í˜¸ ì „ì†¡
        self.emit_gui_signal_if_available("translation_started")
        self.logger.info("âœ… ë²ˆì—­ ì‹œì‘")

        # ë²ˆì—­ ì§„í–‰ í‘œì‹œ ë°” ì‹œì‘
        if hasattr(self, 'gui_signals') and hasattr(self.gui_signals, 'translation_started'):
            self.gui_signals.translation_started.emit()
        
        # --- AWS ëª¨ë“œ ë¶„ê¸° ---
        if self.translation_mode == "aws":
            # ì‹¤ì‹œê°„ ì „ìš© ìŠ¤íŠ¸ë¦¬ë° ìŠ¤ë ˆë“œìš© í”Œë˜ê·¸
            self.aws_stream_stop = threading.Event()
            # AWS ìŠ¤íŠ¸ë¦¬ë° STT â†’ ìµœì¢… í…ìŠ¤íŠ¸ ì–»ê¸°
            self.logger.info("AWS STT ì‹œì‘")
            stt_text = await self._aws_streaming_transcribe(audio_file_path)
            self.logger.info(f"AWS STT ê²°ê³¼: {stt_text}")
            if not stt_text:
                self.logger.warning("âš ï¸ AWS STT ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
        
            # AWS Translate í˜¸ì¶œ
            aws_resp = self.aws_translate.translate_text(
                Text=stt_text,
                SourceLanguageCode=self.aws_source_lang_code,   
                TargetLanguageCode=self.aws_target_lang_code
            )
            self.logger.info(f"ğŸ”„ AWS Translate ì‘ë‹µ: {aws_resp}")
            translated_text = aws_resp["TranslatedText"]

            # GUIì— ì›ë¬¸Â·ë²ˆì—­ ë‚´ë³´ë‚´ê¸°
            self._emit_stt_original(stt_text)  # 2) ì½”ë“œì˜ print(new_text) ëŒ€ì‘
            # translations dict í˜•ì‹ìœ¼ë¡œ ë§ì¶”ê¸°
            translations = { self.translate_target_lang_name: translated_text }
            self.process_translation_result(
                translations,
                stt_text,
                prev_translation="",
                accumulated_text="",
                speech_id=speech_id
            )

            self.logger.info(f"âœ… ì›ë¬¸(AWS): {stt_text}")
            self.logger.info(f"âœ… ë²ˆì—­(AWS): {translated_text}")
            return
        else:
            # --- OpenAI(server/local) ëª¨ë“œ (ê¸°ì¡´ ë¡œì§) ---
            # ìŒì„± ì¶”ì¶œ
            self.logger.info("8080 ì—°ê²°")
            try:
                transcription, translations = self.transcribe_audio(audio_file_path, 8080)
            except:
                self.logger.info(f"audio_file_path: {audio_file_path}")
            # self.logger.info("ë²ˆì—­ ê²°ê³¼ ë°›ìŒ")
            
            if transcription is None:
                return
            if translations is None:
                # ë²ˆì—­ ì²˜ë¦¬
                lang_map = {'ko': 'Korean', 'zh': 'Chinese', 'en': 'English'}
                translations = {'Korean': '', 'Chinese': '', 'English': ''}

                # ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­
                translations = await self._perform_translation(transcription)
                if self.detected_language in lang_map:
                    translations.setdefault(lang_map[self.detected_language], transcription)
                
            # --- GUIì— ì›ë¬¸Â·ë²ˆì—­ ë‚´ë³´ë‚´ê¸° ---         
            self._emit_stt_original(transcription) 
            self.process_translation_result(translations, transcription, "", "", speech_id)
            self.logger.info(f"âœ… ì›ë¬¸: {transcription}")
            
            for k, v in translations.items():
                self.logger.info(f"âœ… {k} ë²ˆì—­: {v}")
            
            
    async def _perform_translation(self, transcription):
        """í…ìŠ¤íŠ¸ ë²ˆì—­ ìˆ˜í–‰"""
        translation_start = datetime.now()
        translations = await self.translate_text_async(transcription)
        translation_end = datetime.now()

        self.logger.info(f"âœ… ë²ˆì—­ ì¢…ë£Œ! ë²ˆì—­ ì‹œê°„: {(translation_end - translation_start).total_seconds():.2f}ì´ˆ")
        return translations
    
    def start_realtime_transcription_loop(self):
        """ì‹¤ì‹œê°„ ëˆ„ì  ìë§‰ ë£¨í”„"""
        if self.translation_mode=="aws":
            if self.translation_mode != "aws":
                # ê¸°ì¡´ OpenAI ë£¨í”„ ìœ ì§€
                threading.Thread(target=self._openai_realtime_loop, daemon=True).start()
            else:
                # AWS ì „ìš© ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë£¨í”„
                threading.Thread(target=lambda: asyncio.run(self._aws_realtime_stream()), daemon=True).start()
        else:
            def loop():
                prev_text = ""
                while self.is_running:
                    time.sleep(1.0)  # 1ì´ˆë§ˆë‹¤ ì‹¤í–‰

                    if not self.voice_detected:
                        prev_text = ""
                        continue

                    with self.buffer_lock:
                        if len(self.audio_frames) < 3:
                            continue
                        frames_copy = list(self.audio_frames)

                    temp_wav = self.save_audio_to_wav(frames_copy)
                    print("8082 ì—°ê²°")
                    transcription, _ = self.transcribe_audio(temp_wav, 8082)
                
                    if transcription:
                        # ì´ì „ ìë§‰ê³¼ ë¹„êµí•´ì„œ ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                        if transcription.startswith(prev_text):
                            new_text = transcription[len(prev_text):]
                        else:
                            new_text = transcription
                        prev_text = transcription

                        # ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„ë§Œ ìë§‰ì— ë„ì›€ (ë˜ëŠ” ì „ì²´ë¥¼ ë„ì›Œë„ ë¨)
                        if new_text.strip():
                            self._emit_stt_original(new_text.strip())
                            print(f"ì‹¤ì‹œê°„ ë²ˆì—­(ì¶”ê°€): {new_text.strip()}")
            threading.Thread(target=loop, daemon=True).start()    
        
    
    async def _aws_streaming_loop(self):
        """í•˜ë‚˜ì˜ WS ì—°ê²°ì—ì„œ send/receiveë¥¼ ë³‘ë ¬ ì‹¤í–‰"""
        async with websockets.connect(self.aws_url, ping_timeout=None) as ws:
            send_task = asyncio.create_task(self._aws_send(ws))
            recv_task = asyncio.create_task(self._aws_receive(ws))
            # í•˜ë‚˜ë¼ë„ ëë‚˜ë©´ ë‚˜ë¨¸ì§€ë¥¼ ì·¨ì†Œ
            done, pending = await asyncio.wait(
                [send_task, recv_task],
                return_when=asyncio.FIRST_COMPLETED
            )
            # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í”Œë˜ê·¸ ì„¤ì •
            self.aws_stream_stop.set()
            
            # ì•„ì§ ì•ˆ ëë‚œ íƒœìŠ¤í¬ëŠ” ì·¨ì†Œ
            for task in pending:
                task.cancel()
            return 

    async def _aws_send(self, ws):
        """ë§ˆì´í¬ ë²„í¼ì˜ ì²­í¬ë¥¼ AWSì— ì „ì†¡"""
        loop = asyncio.get_running_loop()
        while not self.aws_stream_stop.is_set():
            try:
                # íì—ì„œ ìµœëŒ€ 1ì´ˆ ëŒ€ê¸° í›„ ì²­í¬ë¥¼ ê°€ì ¸ì˜´
                chunk = await loop.run_in_executor(
                    None, self.audio_queue.get, True, 1.0
                )
                await ws.send(create_audio_event(chunk))
            except (queue.Empty, asyncio.TimeoutError):
                continue
            except websockets.exceptions.ConnectionClosedOK:
                # WSê°€ ì •ìƒ ì¢…ë£Œëœ ê²½ìš°, ë” ì´ìƒ ë³´ë‚´ì§€ ì•ŠìŒ
                break
            except RuntimeError:
                # ì´ë²¤íŠ¸ë£¨í”„ê°€ ì¢…ë£Œëœ ê²½ìš° (shutdown) ë¹ ì ¸ë‚˜ê°
                break

    async def _aws_receive(self, ws):
        """AWSë¡œë¶€í„° partial/final ì´ë²¤íŠ¸ ë°›ì•„ ì²˜ë¦¬"""
        len_stt = 0
        while not self.aws_stream_stop.is_set():
            raw = await ws.recv()
            header, payload = decode_event(raw)
            if header.get(':message-type') != 'event':
                continue

            results = payload.get('Transcript', {}).get('Results', [])
            # ê²°ê³¼ê°€ ë¹„ì–´ ìˆìœ¼ë©´(Alternativesë„ ì—†ìœ¼ë©´) ë¬´ì‹œ
            if not results or not results[0].get('Alternatives'):
                continue

            res = results[0]
            alt = res['Alternatives'][0]
            text = alt.get('Transcript', "")
            is_partial = res.get('IsPartial', False)

            if is_partial:
                # ë¶€ë¶„ ìë§‰: ì´ì „ ê¸¸ì´ ì´í›„ë§Œ ë³´ëƒ„
                if len(text) > len_stt:
                    delta = text[len_stt:]
                    self._emit_gui_realtime_update({}, delta)
                    len_stt = len(text)
            else:
                # ìµœì¢… ìë§‰
                self._emit_stt_original(text)
                # ë²ˆì—­
                resp = self.aws_translate.translate_text(
                    Text=text,
                    SourceLanguageCode=self.aws_source_lang_code,
                    TargetLanguageCode=self.aws_target_lang_code
                )
                translated = resp.get('TranslatedText', '')
                self.process_translation_result(
                    {self.translate_target_lang_name: translated},
                    text, "", "", speech_id=0
                )
                # ë‹¤ìŒ ë°œí™”ë¥¼ ìœ„í•´ ìƒíƒœ ì´ˆê¸°í™”
                len_stt = 0

    async def _aws_realtime_stream(self):
        """
        AWS Transcribe WebSocket ìŠ¤íŠ¸ë¦¬ë°ì—ì„œ
        partial/complete Transcript ì´ë²¤íŠ¸ë§ˆë‹¤
        new_textë¥¼ ì˜ë¼ë‚´ì–´ GUIì— ë¿Œë ¤ì¤ë‹ˆë‹¤.
        """
        

        try:
            async with websockets.connect(self.aws_url, ping_timeout=None) as ws:
                len_stt_text = 0
                while self.is_running:
                    try:
                        raw = await ws.recv()
                    except websockets.exceptions.ConnectionClosedOK:
                        # ì •ìƒ ì¢…ë£Œ(1000) ì‹œ ì¡°ìš©íˆ ë£¨í”„ íƒˆì¶œ
                        self.logger.info("â„¹ï¸ AWS ìŠ¤íŠ¸ë¦¬ë° ì—°ê²°ì´ ì •ìƒ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                        break
                    except websockets.exceptions.ConnectionClosedError as e:
                        self.logger.warning(f"âš ï¸ AWS ìŠ¤íŠ¸ë¦¬ë° ë¹„ì •ìƒ ì¢…ë£Œ: {e}")
                        break
 
                    header, payload = decode_event(raw)
                    if header.get(':message-type') == 'event':
                        results = payload['Transcript']['Results']
                        if results:
                            stt_text = results[0]['Alternatives'][0]['Transcript']
                            # ë¶€ë¶„/ì™„ë£Œ ì²˜ë¦¬â€¦
 
                    await asyncio.sleep(0.05)
        except Exception as e:
            self.logger.error(f"AWS ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            
        

    def _openai_realtime_loop(self):
        """ê¸°ì¡´ start_realtime_transcription_loopì˜ OpenAI ë¶„ê¸° ë¡œì§"""
        prev_text = ""
        while self.is_running:
            time.sleep(1.0)
            if not self.voice_detected:
                prev_text = ""
                continue
            with self.buffer_lock:
                if len(self.audio_frames) < 3:
                    continue
                frames_copy = list(self.audio_frames)
            temp_wav = self.save_audio_to_wav(frames_copy)
            transcription, _ = self.transcribe_audio(temp_wav, 8082)
            if transcription:
                if transcription.startswith(prev_text):
                    new_text = transcription[len(prev_text):]
                else:
                    new_text = transcription
                prev_text = transcription
                if new_text.strip():
                    self._emit_gui_realtime_update({}, new_text)
                    
    
    # ---------- GUI ì—…ë°ì´íŠ¸ ----------
    def log_timing_on_end_of_speech(self):
        if hasattr(self, 'transcribe_start_time') and self.transcribe_start_time:
            end_time = datetime.now()
            duration = (end_time - self.transcribe_start_time).total_seconds()
            self.logger.info(f"âœ… ë°œí™” ì¢…ë£Œ! ë°œí™” ì‹œê°„: {duration:.2f}ì´ˆ")
            
            
    def set_gui_signals(self, signals):
        """GUI ì‹ í˜¸ ê°ì²´ ì„¤ì •"""
        # self.logger.debug("GUI ì‹ í˜¸ ê°ì²´ ì„¤ì •ë¨")
        self.gui_signals = signals
    
    def emit_gui_signal_if_available(self, signal_name):
        if hasattr(self, 'gui_signals'):
            signal = getattr(self.gui_signals, signal_name, None)
            if signal:
                signal.emit()

    def _emit_stt_original(self, original_text):
        if hasattr(self, 'gui_signals') and hasattr(self.gui_signals, 'stt_original_update'):
            self.gui_signals.stt_original_update.emit(original_text or "")
        else:
            self.logger.warning("âš ï¸ stt_original_update ì‹œê·¸ë„ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ.")
            

    def _emit_gui_realtime_update(self, translations, transcription):
        """GUIì— ì‹¤ì‹œê°„ ë²ˆì—­ ê²°ê³¼ ì „ì†¡"""
        timestamp = datetime.now().strftime('%H:%M:%S')
        gui_message = self._prepare_gui_message(translations, transcription)

        if hasattr(self, 'gui_signals'):
            self.gui_signals.translation_update.emit(
                timestamp,
                json.dumps(gui_message, ensure_ascii=False),
                transcription
            )

    # ---------- ê¸°íƒ€ ---------- 
    def start_threads(self):
        """ì˜¤ë””ì˜¤ ìº¡ì²˜ ë° ë²ˆì—­ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìŠ¤ë ˆë“œ ì‹œì‘"""
        # ì˜¤ë””ì˜¤ ìº¡ì²˜ ìŠ¤ë ˆë“œ
        capture_thread = threading.Thread(
            target=self.audio_capture,
            args=(self.selected_device,),
            daemon=True,
            name="audio_capture_thread"
        )
        capture_thread.start()


        # ë¹„ë™ê¸° ë²ˆì—­ í ì²˜ë¦¬ ìŠ¤ë ˆë“œ
        if self.translation_mode == "aws":
            threading.Thread(
                target=lambda: asyncio.run(self._aws_streaming_loop()),
                daemon=True,
                name="aws_stream"
            ).start()
            
        else:
            # AWS ëª¨ë“œ ì•„ë‹ˆë©´ ê¸°ì¡´ í-ë°°ì¹˜ ìŠ¤ë ˆë“œ
            threading.Thread(
                target=lambda: asyncio.run(self.process_translation_queue()),
                daemon=True,
                name="translation_queue_thread"
            ).start()
        
        self.start_realtime_transcription_loop()
        
        self.logger.info("ì¼ë°˜ ë²ˆì—­ ëª¨ë“œ ìŠ¤ë ˆë“œ ì‹œì‘...")

        self.logger.info("ì˜¤ë””ì˜¤ ìº¡ì²˜ ìŠ¤ë ˆë“œ ì‹œì‘...")